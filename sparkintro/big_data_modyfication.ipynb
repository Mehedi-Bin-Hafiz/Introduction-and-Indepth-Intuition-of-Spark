{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c5eefe0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----+-----+\n",
      "|    date|store|item|sales|\n",
      "+--------+-----+----+-----+\n",
      "|01/01/13|    1|   1|   13|\n",
      "|02/01/13|    1|   1|   11|\n",
      "|03/01/13|    1|   1|   14|\n",
      "|04/01/13|    1|   1|   13|\n",
      "|05/01/13|    1|   1|   10|\n",
      "|06/01/13|    1|   1|   12|\n",
      "|07/01/13|    1|   1|   10|\n",
      "|08/01/13|    1|   1|    9|\n",
      "|09/01/13|    1|   1|   12|\n",
      "|10/01/13|    1|   1|    9|\n",
      "+--------+-----+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "session = SparkSession.builder.appName('bigdata').getOrCreate()\n",
    "df = session.read.csv('dataset/bigdata.csv',header=True,inferSchema= True)\n",
    "df.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f9055e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "913000"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## count row\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fdecbf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- store: integer (nullable = true)\n",
      " |-- item: integer (nullable = true)\n",
      " |-- sales: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9e74d9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'store', 'item', 'sales']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.na.drop(how='any')\n",
    "df.count()\n",
    "col_name = df.columns\n",
    "col_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07160c41",
   "metadata": {},
   "source": [
    "## we need to modify date field by Replace Column Value Character by Character (/ by -)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a19da",
   "metadata": {},
   "source": [
    "By using translate() string function you can replace character by character of DataFrame column value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "353ca9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----+-----+\n",
      "|    date|store|item|sales|\n",
      "+--------+-----+----+-----+\n",
      "|01-01-13|    1|   1|   13|\n",
      "|02-01-13|    1|   1|   11|\n",
      "|03-01-13|    1|   1|   14|\n",
      "|04-01-13|    1|   1|   13|\n",
      "|05-01-13|    1|   1|   10|\n",
      "|06-01-13|    1|   1|   12|\n",
      "|07-01-13|    1|   1|   10|\n",
      "|08-01-13|    1|   1|    9|\n",
      "|09-01-13|    1|   1|   12|\n",
      "|10-01-13|    1|   1|    9|\n",
      "+--------+-----+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###very important code\n",
    "from pyspark.sql.functions import translate\n",
    "df = df.withColumn('date', translate('date', '/', '-'))\n",
    "df.show(n=10)\n",
    "#This unix_timestamp support only dd-mm-yy ex (02-12-21). but our row data was some of them this format somr of them\n",
    "#10/12/2021 format. so at frist I fortamt this to 10/12/2021 format in excel. then I replaced / by - using this line. then \n",
    "# I apply from_unixtime funsion and change to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2eb9f767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----+-----+-------------------+\n",
      "|    date|store|item|sales|           datetime|\n",
      "+--------+-----+----+-----+-------------------+\n",
      "|01-01-13|    1|   1|   13|2013-01-01 00:01:00|\n",
      "|02-01-13|    1|   1|   11|2013-01-02 00:01:00|\n",
      "|03-01-13|    1|   1|   14|2013-01-03 00:01:00|\n",
      "|04-01-13|    1|   1|   13|2013-01-04 00:01:00|\n",
      "|05-01-13|    1|   1|   10|2013-01-05 00:01:00|\n",
      "|06-01-13|    1|   1|   12|2013-01-06 00:01:00|\n",
      "|07-01-13|    1|   1|   10|2013-01-07 00:01:00|\n",
      "|08-01-13|    1|   1|    9|2013-01-08 00:01:00|\n",
      "|09-01-13|    1|   1|   12|2013-01-09 00:01:00|\n",
      "|10-01-13|    1|   1|    9|2013-01-10 00:01:00|\n",
      "+--------+-----+----+-----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#seperate datetime\n",
    "from pyspark.sql.functions import year\n",
    "from pyspark.sql.functions import to_timestamp,date_format\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "\n",
    "df2 = df.select(\n",
    "    'date', 'store', 'item', 'sales', \n",
    "    from_unixtime(unix_timestamp('date', 'dd-mm-yy')).alias('datetime')\n",
    ")\n",
    "df2.show(n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4a101a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- store: integer (nullable = true)\n",
      " |-- item: integer (nullable = true)\n",
      " |-- sales: integer (nullable = true)\n",
      " |-- datetime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3cca31",
   "metadata": {},
   "source": [
    "## Separate datetime : day ,month, year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5da160b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----+-----+-------------------+-----+----+\n",
      "|    date|store|item|sales|           datetime|month|year|\n",
      "+--------+-----+----+-----+-------------------+-----+----+\n",
      "|01-01-13|    1|   1|   13|2013-01-01 00:01:00|    1|2013|\n",
      "|02-01-13|    1|   1|   11|2013-01-02 00:01:00|    1|2013|\n",
      "|03-01-13|    1|   1|   14|2013-01-03 00:01:00|    1|2013|\n",
      "|04-01-13|    1|   1|   13|2013-01-04 00:01:00|    1|2013|\n",
      "|05-01-13|    1|   1|   10|2013-01-05 00:01:00|    1|2013|\n",
      "|06-01-13|    1|   1|   12|2013-01-06 00:01:00|    1|2013|\n",
      "|07-01-13|    1|   1|   10|2013-01-07 00:01:00|    1|2013|\n",
      "|08-01-13|    1|   1|    9|2013-01-08 00:01:00|    1|2013|\n",
      "|09-01-13|    1|   1|   12|2013-01-09 00:01:00|    1|2013|\n",
      "|10-01-13|    1|   1|    9|2013-01-10 00:01:00|    1|2013|\n",
      "|11-01-13|    1|   1|    9|2013-01-11 00:01:00|    1|2013|\n",
      "|12-01-13|    1|   1|    7|2013-01-12 00:01:00|    1|2013|\n",
      "|13-01-13|    1|   1|   10|2013-01-13 00:01:00|    1|2013|\n",
      "|14-01-13|    1|   1|   12|2013-01-14 00:01:00|    1|2013|\n",
      "|15-01-13|    1|   1|    5|2013-01-15 00:01:00|    1|2013|\n",
      "|16-01-13|    1|   1|    7|2013-01-16 00:01:00|    1|2013|\n",
      "|17-01-13|    1|   1|   16|2013-01-17 00:01:00|    1|2013|\n",
      "|18-01-13|    1|   1|    7|2013-01-18 00:01:00|    1|2013|\n",
      "|19-01-13|    1|   1|    7|2013-01-19 00:01:00|    1|2013|\n",
      "|20-01-13|    1|   1|   15|2013-01-20 00:01:00|    1|2013|\n",
      "+--------+-----+----+-----+-------------------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.withColumn('month',month(df2.datetime)).withColumn('year',year(df2.datetime))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "83a102c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'store', 'item', 'sales', 'datetime', 'month', 'year']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c3ba54d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----+-----+-------------------+-----+----+---+\n",
      "|    date|store|item|sales|           datetime|month|year|day|\n",
      "+--------+-----+----+-----+-------------------+-----+----+---+\n",
      "|01-01-13|    1|   1|   13|2013-01-01 00:01:00|    1|2013|  1|\n",
      "|02-01-13|    1|   1|   11|2013-01-02 00:01:00|    1|2013|  2|\n",
      "|03-01-13|    1|   1|   14|2013-01-03 00:01:00|    1|2013|  3|\n",
      "|04-01-13|    1|   1|   13|2013-01-04 00:01:00|    1|2013|  4|\n",
      "|05-01-13|    1|   1|   10|2013-01-05 00:01:00|    1|2013|  5|\n",
      "|06-01-13|    1|   1|   12|2013-01-06 00:01:00|    1|2013|  6|\n",
      "|07-01-13|    1|   1|   10|2013-01-07 00:01:00|    1|2013|  7|\n",
      "|08-01-13|    1|   1|    9|2013-01-08 00:01:00|    1|2013|  8|\n",
      "|09-01-13|    1|   1|   12|2013-01-09 00:01:00|    1|2013|  9|\n",
      "|10-01-13|    1|   1|    9|2013-01-10 00:01:00|    1|2013| 10|\n",
      "+--------+-----+----+-----+-------------------+-----+----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Easy way to retrive day\n",
    "\n",
    "df2 = df2.select(\n",
    "    'date', 'store', 'item', 'sales', 'datetime', 'month', 'year', \n",
    "    dayofmonth(\"datetime\").alias('day')\n",
    ")\n",
    "df2.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb09430",
   "metadata": {},
   "source": [
    " ## Separate datetime : day ,month, year (Easy Way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac533d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initailize SparkSession and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bffe4680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'store', 'item', 'sales']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.na.drop(how='any')\n",
    "df.count()\n",
    "col_name = df.columns\n",
    "col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "240b497d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----+-----+\n",
      "|    date|store|item|sales|\n",
      "+--------+-----+----+-----+\n",
      "|01-01-13|    1|   1|   13|\n",
      "|02-01-13|    1|   1|   11|\n",
      "|03-01-13|    1|   1|   14|\n",
      "|04-01-13|    1|   1|   13|\n",
      "|05-01-13|    1|   1|   10|\n",
      "|06-01-13|    1|   1|   12|\n",
      "|07-01-13|    1|   1|   10|\n",
      "|08-01-13|    1|   1|    9|\n",
      "|09-01-13|    1|   1|   12|\n",
      "|10-01-13|    1|   1|    9|\n",
      "+--------+-----+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###very important code\n",
    "from pyspark.sql.functions import translate\n",
    "df = df.withColumn('date', translate('date', '/', '-'))\n",
    "df.show(n=10)\n",
    "#This unix_timestamp support only dd-mm-yy ex (02-12-21). but our row data was some of them this format somr of them\n",
    "#10/12/2021 format. so at frist I fortamt this to 10/12/2021 format in excel. then I replaced / by - using this line. then \n",
    "# I apply from_unixtime funsion and change to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d37c5619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----+-----+-------------------+\n",
      "|    date|store|item|sales|           datetime|\n",
      "+--------+-----+----+-----+-------------------+\n",
      "|01-01-13|    1|   1|   13|2013-01-01 00:01:00|\n",
      "|02-01-13|    1|   1|   11|2013-01-02 00:01:00|\n",
      "|03-01-13|    1|   1|   14|2013-01-03 00:01:00|\n",
      "|04-01-13|    1|   1|   13|2013-01-04 00:01:00|\n",
      "|05-01-13|    1|   1|   10|2013-01-05 00:01:00|\n",
      "|06-01-13|    1|   1|   12|2013-01-06 00:01:00|\n",
      "|07-01-13|    1|   1|   10|2013-01-07 00:01:00|\n",
      "|08-01-13|    1|   1|    9|2013-01-08 00:01:00|\n",
      "|09-01-13|    1|   1|   12|2013-01-09 00:01:00|\n",
      "|10-01-13|    1|   1|    9|2013-01-10 00:01:00|\n",
      "+--------+-----+----+-----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#seperate datetime\n",
    "from pyspark.sql.functions import year\n",
    "from pyspark.sql.functions import to_timestamp,date_format\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "\n",
    "df2 = df.select(\n",
    "    'date', 'store', 'item', 'sales', \n",
    "    from_unixtime(unix_timestamp('date', 'dd-mm-yy')).alias('datetime')\n",
    ")\n",
    "df2.show(n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "89067724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----+-----+---+-----+----+-----+\n",
      "|    date|           datetime|year|month|day|store|item|sales|\n",
      "+--------+-------------------+----+-----+---+-----+----+-----+\n",
      "|01-01-13|2013-01-01 00:01:00|2013|    1|  1|    1|   1|   13|\n",
      "|02-01-13|2013-01-02 00:01:00|2013|    1|  2|    1|   1|   11|\n",
      "|03-01-13|2013-01-03 00:01:00|2013|    1|  3|    1|   1|   14|\n",
      "|04-01-13|2013-01-04 00:01:00|2013|    1|  4|    1|   1|   13|\n",
      "|05-01-13|2013-01-05 00:01:00|2013|    1|  5|    1|   1|   10|\n",
      "|06-01-13|2013-01-06 00:01:00|2013|    1|  6|    1|   1|   12|\n",
      "|07-01-13|2013-01-07 00:01:00|2013|    1|  7|    1|   1|   10|\n",
      "|08-01-13|2013-01-08 00:01:00|2013|    1|  8|    1|   1|    9|\n",
      "|09-01-13|2013-01-09 00:01:00|2013|    1|  9|    1|   1|   12|\n",
      "|10-01-13|2013-01-10 00:01:00|2013|    1| 10|    1|   1|    9|\n",
      "+--------+-------------------+----+-----+---+-----+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df = df2.select(\n",
    "    'date','datetime',\n",
    "    year(\"datetime\").alias('year'), \n",
    "    month(\"datetime\").alias('month'),\n",
    "    dayofmonth(\"datetime\").alias('day'),\n",
    "    'store', 'item', 'sales',\n",
    ")\n",
    "final_df.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "80aba191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "913000"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "21320650",
   "metadata": {},
   "outputs": [],
   "source": [
    "### export data to csv\n",
    "final_df.toPandas().to_csv('dataset/modyfiedBigdata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9ddfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f6d03f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741efb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d373b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
